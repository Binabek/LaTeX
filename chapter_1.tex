\chapter{Introduction} \label{chap:intro}

\section{Background} \label{sec:back}
Charged particle accelerators have been productive experimental tools for fundamental physics experiments, from Rutherford's use of naturally accelerated alpha particles for the discovery of the nucleus to the contemporary multinational physics collaborations centered at the Large Hadron Collider.  In addition to their useful nature as tools, particle accelerators provide an interesting system to study dynamics in a controlled environment. These same dynamical studies then have direct impacts on the construction of new machines for fundamental studies. As the experiments drive increasing energy and power demands on the beams, careful control of losses becomes more important as ever smaller loss fractions exceed safety and machine protection limits.

To tackle the challenge of controlling energetic subatomic particles we first have to choose our tools. Some of the earliest "accelerator" experiments relied on energetic decay products, leveraging the weak nuclear force. This approach is quickly limited by the characteristic energies of these decays. Gravity can be easily dismissed as too weak and effectively fixed on the earth's surface. Bulk material interactions can be used to affect the path of particles, but they cannot accelerate and tend to cause significant losses. The default choice has been electromagnetic fields, dating back to early cathode ray experiments. These fields are easy to produce and control and can both steer and provide energy to charged particles. Naturally this restricts our ability to work with neutral particle beams, but this can be typically overcome with a charged primary beam to produce a neutral secondary beam as is the case in successful neutron spallation and neutrino beam facilities \cite{hendersonSpallationNeutronSource2014} \cite{adamsonNuMINeutrinoBeam2016}.

Scaling the electrostatic fields from our early cathode ray tubes are a straightforward starting point frr accelerating particles. A large voltage applied across carefully shaped electrodes can simultaneously accelerate and focus a beam. This was the guiding principle for early Van de Graaff machines like the Westinghouse atom smasher and the Cockroft-Walton style proton sources popularly used in the mid 20th century. This approach still finds use in pellotrons and "tandem" accelerators for low energy nuclear experimentation. However, the practical limits of breakdown gradients quickly limit the energy from such devices. Using an oscillating electric field is a practical approach to bypassing this limitation in two ways. First, the gradients of oscillating fields may be much higher than static fields. Second, by selectively timing the particles to be accelerated, the particle can gain energy from the gradient at the frequency of the oscillation. There are a few ways this is currently accomplished. The first is simply physically shielding the particles from the negative gradient, as is done in the Alvarez or drift tube linear accelerator (linac). If the velocity of a traveling EM wave is matched to that of the desired accelerated particle the beam can always see a positive gradient and "surf" the wave. This is the approach of a traveling wave linac, typified by the original SLAC linac. Another approach is to adjust the path length of the beam between standing wave structures (usually called cavities) and the phase of these cavities to always arrive in a positive gradient. There are two main approaches using this technique. Either a sequence of cavities with proper phase are used sequentially, a resonant cavity linac. Or, the particles are recirculated to a cavity many times, the case with a broad variety of cyclic machines.

So far the problem of imparting energy to the particles has been given precedence, but naturally the particles must also be steered and focused for useful purposes. Here we can consider magnetic fields as well as electric. While not possible to be used for energy gain, magnetic fields are the preferred option for steering and control. While electrostatic fields are used in some low energy applications, the proportional scaling of the force with momentum from a magnetic field pay dividends as beam energy increases and is the dominant approach for accelerator applications. We can consider then the earliest cyclic machine, the cyclotron. This consists of a single resonant electric gap in a static perpendicular magnetic field. By properly scaling the field strength, in the nonrelatavistic approximation, the classical cyclotron frequency means that the particle will continue to cycle at the correct frequency to see a positive gradient and gain energy. As a result, a modest field on the gap seen many times can result in a significant energy gain for the beam. The trade off is that now we have a periodic system which must support stable motion in the plane transverse to the direction of acceleration. This is not unique to cyclic machines, but the stability conditions are less stringent in linear machines, as they are one pass.

The energy limitations imposed by the relativistic effects in fixed field cyclotrons straightforwardly motivated ramping the magnetic field in time with the nominal energy to produced a fixed reference trajectory. This breaks the isochronous condition and introduces a synchronous frequency for a given design energy. The resulting "synchrotrons" represent the current standard in high-energy accelerators, holding the absolute beam energy record since the late 60's. Advantages in alternating gradient focusing and later separated function magnets resulted in greatly reduced beam sizes and increased control over the dynamics. However, as intensities and energies increased, the requirements on the beam control increased commensurately. Higher energies required close control of chromaticity to suppress head-tail instabilities, and elements to provide for landau damping to suppress general collective instabilities all add nonlinear perturbations which serve to limit the stable region of dynamics. In high intensity machines, self forces in the form of space charge serve to significantly perturb the dynamics and increase vulnerability to parametric resonances. These shortcomings have long motivated searches for nonlinear integrable focusing systems in accelerators. Such a potential has been implemented at the Integrable Optics Test Accelerator (IOTA). This thesis describes the system, and presents experimental results on studies of the practicality of the nonlinear focusing system.

\section{General Beamline Hamiltonian} \label{sec:genHam}

This problem of stability is of course not unique to accelerators and has a long history of study of dynamical systems. Accordingly, we can construct a Hamiltonian of the system. To evaluate the inherent stability of the system we will construct the Hamiltonian for a single particle in external confining potentials in two dimensions. Notably this does not include the self fields of the particle, or of the interactions of many particles in a beam. We are specifically interested in a Hamiltonian which describes a relativistic particle in electromagnetic fields. This results in the general Hamiltonian \ref{eq:H_em} where $\Phi$ and $\vec{A}$ are the typical scalar and vector potentials related to the Electric and Magnetic fields by \ref{eq:phi_A}

\begin{equation} \label{eq:H_em}
H_{EM} = E_{\Phi} = c\sqrt{(\vec{P} - q\vec{A}) + m^2 c^2} + q\Phi
\end{equation}

\begin{equation} \label{eq:phi_A}
\begin{split} 
\vec{E} &= -\nabla \Phi - \frac{\partial \vec{A}}{\partial t}\\
\vec{B} &= \nabla \times \vec{A}
\end{split}
\end{equation}

Here the canonical momentum $\vec{P}$ is related to the mechanical momentum by \ref{eq:P_mv}

\begin{equation} \label{eq:P_mv}
	\vec{P} = \gamma m\vec{v} - q \vec{A}
\end{equation}

And this Hamiltonian is then the total energy of the particle including the contribution from the scalar potential, I will call this $E_{\Phi}$ to differentiate it from the total energy in the absence of the scalar potential.

The following derivations for the equations of motion generally follow the tradition of the solution found in \cite{courantTheoryAlternatinggradientSynchrotron1958}, and generally follow the approach outlined in \cite{wolskiBeamDynamicsHigh2014} with some elements from \cite{leeAcceleratorPhysics2018}. We would like to refine this expression by changing to a more useful coordinate system. When studying the dynamics of a particle accelerator it is useful to investigate the motion of the particles with respect to some reference trajectory rather their absolute coordinates in the lab frame. To start with, we adopt a Frenet-Serret (Fig. \ref{fig:frenetSerret}) coordinate system where the coordinate axes are defined with respect to a tangent vector of an arbitrary curved path.  In the case of many accelerator systems, the situation is simpler than this. In practice, bending is only in a single plane, typically horizontal in the lab frame along the floor. Additionally, if bending is only generated from the motion of charged particles in a static magnetic field, the bent path will be circles of a characteristic bending radius $\rho$. The result of this transformation only ends up modifying the momentum along the curved trajectory by a factor of $1 + \frac{x}{\rho}$ and gives us the Hamiltonian $H_{f.s.}$ in \ref{eq:H_fs}. This coordinate transformation does not affect the value of the Hamiltonian, so it is still equivalent to the total energy of the test particle with the scalar field contribution. Strictly speaking this is the case for transfer lines, linacs, synchrotrons and storage rings. There are other style machines, such as cyclotrons and fixed field alternating gradient accelerators which demand a different treatment of the dynamics, but are beyond the scope of this derivation.

\begin{figure} 
	\centering
	\includegraphics[width=0.8\linewidth]{madxFrenetSerret.pdf}
	\caption{Illustration of Frenet-Serret style Coordinate System From Mad X Documentation \cite{deniauMADXProgramUsers}}
	\label{fig:frenetSerret}
\end{figure}

\begin{equation} \label{eq:H_fs}
	H_{f.s.} = E_{\Phi} = c\sqrt{\left(\frac{p_s}{1 + x/\rho} - qA_s\right)^2 + (P_x - qA_x)^2 + (P_y- qA_y)^2+ m^2c^2} + q\Phi
\end{equation}

Since an accelerator is composed of sequential elements along this reference trajectory, it is useful to change the independent variable to the spacial coordinate along the trajectory. Inspecting the action with respect to the s dimension shows defining the new Hamiltonian as $H_s(x,p_x,y,p_y,t,-E_{\Phi};s) = -p_s$ only requires rearranging Eq. \ref{eq:H_fs} to arrive at Eq. \ref{eq:H_s}.

\begin{multline} \label{eq:H_s}
	H_s = -\left(1 + \frac{x}{\rho}\right)\sqrt{\left(\frac{E_{\Phi} - q\Phi}{c}\right)^2 - (P_x -q A_x)^2 - (P_y - q A_y)^2 - m^2c^2}\\ - \left(1 + \frac{x}{\rho}\right)qA_s
\end{multline}

There is one more coordinate transformation we would like to make. Currently our longitudinal canonical coordinate is time, which will increase quickly along the beam line and become cumbersome. We would like to define the position of the beam with respect to some reference particle, which has the ideal quantities we would like in our accelerated beam. Correspondingly we define a particle on our reference trajectory with a momentum $p_o$ (Eq.\ref{eq:p_o}) directed along the reference trajectory.

\begin{equation} \label{eq:p_o}
	p_o = \beta_o \gamma_o m c
\end{equation}

We can then scale the overall Hamiltonian by this reference momentum Eq. \ref{eq:H_p}. The equations of motion are still satisfied with the inclusion of the already substituted variable changes in Eq. \ref{eq:pOverP}.

\begin{multline} \label{eq:H_p}
	H_p = \frac{H_s}{p_o} = - \left(1 + \frac{x}{\rho}\right)\frac{q}{p_o}A_s\\
-\left(1 + \frac{x}{\rho}\right)\sqrt{\left(\frac{\tilde{E}_{\Phi} - \frac{q}{p_o}\Phi}{c}\right)^2 - \left(p_x -\frac{q}{p_o} A_x\right)^2 - \left(p_y - \frac{q}{p_o} A_y\right)^2 - \frac{m^2c^2}{p_o^2}}
\end{multline}


\begin{equation} \label{eq:pOverP}
\begin{split}
	p_x &= \frac{P_x}{p_o}\\
	p_y &= \frac{P_y}{p_o}\\
	\tilde{E_{\Phi}} &= \frac{E_{\Phi}}{p_o}
\end{split}
\end{equation}

The last coordinate transformation we will make is to shift the longitudinal position of the test particle with respect to our reference particle. Using a generating function of the form Eq. \ref{eq:relGen}


\begin{equation} \label{eq:relGen}
	G_2(x,p_x,y,p_y,t,p_t;s) = x p_x  + y p_y + \left(ct - \frac{s}{\beta_o}\right)\left(p_t - \frac{1}{\beta_o}\right)
\end{equation}

The $x,y$ coordinates are not affected by the transformation, but we have the following new longitudinal coordinates in Eq. \ref{eq:statCoords}. Here we follow the direction convention chosen in ImpactX as it will be the native coordinate system for our simulation results later on.

\begin{equation} \label{eq:statCoords}
\begin{split}
	ct &= ct - \frac{s}{\beta_o}\\
	\tilde{p_t} &= \frac{1}{\beta_o} - \frac{\tilde{E_{\Phi}}}{c}\\
	\tilde{H_p} &= H_p - \frac{\tilde{p_t}}{\beta_o} + \frac{1}{\beta_o^2}
\end{split}
\end{equation}

After substitution and removing the constant $\frac{1}{\beta_o^2}$ term we have the Hamiltonian in Eq. \ref{eq:H_A}

\begin{multline} \label{eq:H_A}
	H_A = - \left(1 + \frac{x}{\rho}\right)\frac{q}{p_o}A_s - \frac{\tilde{p_t}}{\beta_o}\\
	-\left(1 + \frac{x}{\rho}\right)\sqrt{\left(\frac{1}{\beta_o} - \tilde{p_t}  - \frac{q}{p_o c}\Phi\right)^2 - \left(p_x -\frac{q}{p_o} A_x\right)^2 - \left(p_y - \frac{q}{p_o} A_y\right)^2 - \frac{m^2c^2}{p_o^2}}
\end{multline}

We then have arrived at the general Hamiltonian for a particle along our reference trajectory with respect to a nominal reference particle in arbitrary electromagnetic fields. 

From here we now need to decide on which fields to apply to our particle for stable acceleration. We are interested in representing the basic accelerator components, i.e. transverse magnets and radiofrequency cavities. For this case we can take $\Phi, A_x, A_y = 0$, which yields the Hamiltonian in \ref{eq:H_bet}. Components that do not meet these conditions, like a solenoid, can be treated analogously to the following approach, but are omitted for brevity.

\begin{equation} \label{eq:H_bet}
	H = -\left(1 + \frac{x}{\rho}\right)\sqrt{\left(\frac{1}{\beta_o} - p_t \right)^2 - p_x^2 - p_y^2- \frac{m^2c^2}{p_o^2}} - \frac{p_t}{\beta_o} - \left(1 + \frac{x}{\rho}\right)\frac{q}{p_o}A_s
\end{equation}

With the scalar potential set to zero, we arrive at a common set of canonical coordinates used for studies of accelerator dynamics (Eq. \ref{eq:coords}). The relevant difference here from our coordinates above is that now the longitudinal momentum $p_t$ is the difference in the relativistic energy (Eq. \ref{eq:relEnergy}) of the test and reference particle. These definitions are now completely consistent with those used in ImpactX. I refer to the longitudinal spacial coordinate as $ct$ to continually remind that the units are in meters.

\begin{equation} \label{eq:coords}
\begin{split}
x &= x\\
p_x &= \frac{P_x}{p_o}\\
y &= y\\
p_y &= \frac{P_y}{p_o}\\
ct &= c(t-t_o)\\
p_t &= \frac{E_o-E}{p_o c}
\end{split}
\end{equation}

\begin{equation} \label{eq:relEnergy}
	E = \gamma m c^2 \Rightarrow E_o = \gamma_o m c^2
\end{equation}

There are two prominent alternative coordinate definitions in use. The first is an alternative longitudinal coordinate parameterization using the difference in momentum instead of energy, usually called $\delta$, which also changes the longitudinal coordinate definition to spacial offset. The relationship between these coordinates and the coordinates in Eq. \ref{eq:coords} is given in Eq. \ref{eq:delCoord} and expanded upon in Appendix \ref{apx:delToPt}.

\begin{equation} \label{eq:delCoord}
\begin{split}
	\delta &= \frac{p-p_o}{p_o} = \sqrt{p_t^2 -\frac{2}{\beta_o}p_t + 1} -1 \approx -\frac{p_t}{\beta_o}\\
	z &= ct\frac{\sqrt{\beta_o^2 + \beta_o^2 p_t^2 - 2\beta_o p_t}}{\beta_o p_t - 1} \approx  - \beta_o c t
\end{split}
\end{equation}

The other coordinate convention which deserves mention is the use of $x' = \frac{dx}{ds}$. This is an easy to understand coordinate, but it is in general non-canonical and so is avoided in our Hamiltonian based approach. This value converges with our definition of $p_x$ in the ultrarelatavistic case.

Recognizing $\frac{m^2c^2}{p_o^2} = \frac{1}{\beta_o^2 \gamma_o^2}$ we can make one more simplification now that $\Phi = 0$, giving us the Hamiltonian in Eq. \ref{eq:H_As}.

\begin{equation} \label{eq:H_As}
	H = -\left(1 + \frac{x}{\rho}\right)\sqrt{1 + p_t^2 - \frac{2 p_t}{\beta_o} - p_x^2 - p_y^2- } - \frac{p_t}{\beta_o} - \left(1 + \frac{x}{\rho}\right)\frac{q}{p_o}A_s
\end{equation}

The exact form of $A_s$ must now be selected to find our equations of motion. Before we do so we will consider the general form of this expansion we are interested in.

\section{Multipole Expansion} \label{sec:multipole}

To determine the fields used, we first consider what is convenient to generate. In general, a beam typically needs an evacuated beam pipe to travel in, so a we would like to consider magnetic fields that we generate outside of a region of beam travel. As a result we can consider a series solution for Laplace's equations for magnetic fields in two dimensions for a region with periodic cylindrical boundary conditions, Eq. \ref{eq:poissonCylinder}

\begin{equation} \label{eq:poissonCylinder}
\begin{split}
	B_r(r,\theta) &= \sum_{n=1}^{\infty} C_n(z) \left( r \right)^{n-1} \sin{(n\theta)}\\
	B_\theta(r,\theta) &= \sum_{n=1}^{\infty} C_n(z) \left( r \right)^{n-1} \cos{(n\theta)}
\end{split}
\end{equation}

We can apply an alternative combined representation with complex variables introduced by Beth \cite{bethComplexRepresentationComputation1966} \ref{eq:bethCyl}. There are two different conventions for numbering the multipole orders. The convention used in this thesis is the "European" convention, the "US" convention shifts down the multipole term by an integer so the exponent simply becomes the summation variable.

\begin{equation} \label{eq:bethCyl}
	B_{\theta} + i B_r = \sum_{n=1}^{\infty} C_n(z) e^{in\theta} \left( r \right)^{n-1}
\end{equation}

Using the substitutions in Eq. \ref{eq:cylToCart} we can convert the field representation to Cartesian coordinates in Eq. \ref{eq:bethCart}. This can then be matched to our Hamiltonian coordinates.

\begin{equation} \label{eq:cylToCart}
\begin{split}
	B_{\theta} + i B_r &= B_y + i B_x\\
	r e^{i\theta} &= x + iy
\end{split}
\end{equation}

\begin{equation} \label{eq:bethCart}
	B_y + iB_x = \sum_{n=1}^{\infty} C_n(z) (x + iy)^{n-1}
\end{equation}

The term $C_n(z)$ contains the multipole terms, and is in general a complex term with real "normal" and complex "skew" terms. The skew fields end up being the normal fields rotated by a factor of $\frac{\pi}{2n}$ for a given order. Figure \ref{fig:multipoles} shows the first few multipole fields, both normal and skew. As there are no magnetic monopoles, each multipole term represents another radially symmetric addition of a pair of north and south poles. The multipole terms are named according to a mix of Latin and Greek numeric prefixes, $n=1$ is dipole, $n=2$ is quadrupole, $n=3$ is a sextupole etc.

\begin{figure} 
	\centering
	\includegraphics[width=1\linewidth]{multipoleExamples.pdf}
	\caption{First three multipoles}
	\label{fig:multipoles}
\end{figure}


Of course, the three dimensional consequence of Gauss's law mean that the magnetic field extends beyond the length of the pole, and has a nonlinear falloff along the beam path. However, In practice we are typically only concerned with the effective integral of the field, the length of the magnets and the fringe field especially are short compared to the characteristic wavelengths of the betatron motion and only become relevant in large aperture magnets with long fringing fields. We can then exclude the $z$ dependence of our multipole terms and arrive at the general definition for our two dimensional multipole expansion in Eq. \ref{eq:mult}. Here the $B_n$ terms are the normal components and the $A_n$ terms are skew. 

\begin{equation} \label{eq:mult}
	B_y + iB_x = \sum_{n=1}^{\infty} (B_n + iA_n) (x + iy)^{n-1}
\end{equation}

There are a few different parameterizations of this multipole expansion including reference radii and reference fields. We will only consider scaling to our so magnetic rigidity in Eq. \ref{eq:brho}. 

\begin{equation} \label{eq:brho}
	B\rho = \frac{p_o}{q}
\end{equation}

This is a useful quantity which indicates the effect of fields on a particle irrelevant of species. For a given fixed magnetic field, the radius of the characteristic circle of its motion is given by the rigidity over the field strength. So, a high rigidity particle will be bent less than a low rigidity particle in the same field. We can then define a multipole scaling to this rigidity in Eq. \ref{eq:multK}. We will only consider normal terms in the derivations later, but the same approach can be done for skew terms.

\begin{equation} \label{eq:multK}
	k_n = \frac{B_n}{B\rho}
\end{equation}

This expansion has been about a straight reference path which contrasts with our earlier transformation to the Frenet-Serret frame. There do exist treatments of multipoles defined in a curved reference frame \cite{zolkinSectorMagnetsTransverse2017}, but this is of relatively minor concern. In practice, we can see that our dipole magnet is the only term which results in bending of the reference trajectory. So, these curved trajectory cases only are relevant inside an element with a deliberate dipole term. This brings up the notion of separated and combined function machines. The first generation of machines used a fixed magnetic field for the entire radius, sometimes with short gaps for injection and extraction. Vertical focus either relied on the field falloff due to realistic magnet construction, or a deliberately added gradient to the median plane, effectively a mixed dipole and quadrupole term in our multipole expansion. First generation alternating gradient accelerators kept this approach, simply implementing two types of "combined function" magnets with consistent dipole terms and alternating quadrupole terms for much stronger net focusing. It quickly became clear that it was preferable to separate the bending and focusing components of the fields for greater flexibility in design. As a result, in most modern machines exhibit bending only in pure dipole magnets and higher order fields are placed in straight sections. Combined function magnets are typically only used when particular cost or space concerns renders them more practical.

An additional consideration for the dipole fields is the incident angle on the element. There are two broad approaches, a rectangular bending magnet, and a sector bend. The sector bend has poles with faces perpendicular to the direction of travel. Without this, there is effective focusing based on the longer and shorter path lengths in the constant field depending on the initial horizontal coordinate. Figure \ref{fig:sector} shows the difference between a sector and rectangular dipole magnet. This can be exploited for focusing effects as was done in the Zero Gradient Synchrotron, but is not relevant for the content of this these as all dipoles considered are sector bends.

\begin{figure} 
	\centering
	\includegraphics[width=1\linewidth]{sectorRect.pdf}
	\caption{Sector and Rectangular bending dipoles}
	\label{fig:sector}
\end{figure}


For the purposes of evaluating our Hamiltonian we can then represent the vector potential with respect to the multipole expansion terms in Eq. \ref{eq:multAs}.

\begin{equation} \label{eq:multAs}
	A_s  =  - \mathbb{Re} \left[ \sum_{n=1}^{\infty} \frac{B_n + i A_n}{n} (x+iy)^n \right]
\end{equation}


\section{Linearized Hamiltonian} \label{sec:linHam}
Based on the form of the multipole expansion, we can see that only our dipole and quadrupole elements will affect the dynamics to the first order. While in general, we can use potentials of any order, we are not guaranteed an analytical solution. The first order solutions to the equations of motion are possible to find analytically and will serve as the foundation for our dynamical analysis moving forward.

We will look for a solution to the equations of motion for a static quadrupole field. This has the vector potential in Eq. \ref{eq:As_B2}.

\begin{equation} \label{eq:As_B2}
	A_s = -\frac{B_2}{2}(x^2 - y^2)
\end{equation}

We also have no bending of the reference trajectory here so $\rho \rightarrow \infty$. Recalling our relationship for the rigidity $B\rho = \frac{p_o}{q}$ the resulting Hamiltonian is Eq. \ref{eq:H_q}

\begin{equation} \label{eq:H_q}
	H = -\sqrt{1 + p_t^2 - \frac{2p_t}{\beta_o} - p_x^2 - p_y^2} - \frac{p_t}{\beta_o} + \frac{k_2}{2}(x^2 - y^2)
\end{equation}

We will now make the first big assumption towards our solution. If we expand the square root term in the Hamiltonian and truncate to lowest order in the dynamical variables (Eq. \ref{eq:sqrt_taylor}) we are making the paraxial approximation. The argument is that since our canonical momentum coordinates are all defined as a ratios with respect to the reference particle, when it is relativistic the motion of these particles is all nearly parallel and these quantities are small, so higher orders can safely be ignored. Naturally this assumption breaks down for low energy beams, but is generally good for most practical accelerators.

\begin{equation} \label{eq:sqrt_taylor}
	\sqrt{1 + p_t^2 - \frac{2p_t}{\beta_o} - p_x^2 - p_y^2} \Rightarrow 1 - \frac{p_t^2}{2\beta_o^2\gamma_o^2} - \frac{p_t}{\beta_o} - \frac{p_x^2}{2} - \frac{p_y^2}{2} + O(3)
\end{equation}

Resubstituting and dropping the constant factor, we arrive at the Hamiltonian in Eq. \ref{eq:H_parax}

\begin{equation} \label{eq:H_parax}
	H = \frac{p_x^2}{2} + \frac{p_y^2}{2} + \frac{p_t^2}{2\beta_o^2\gamma_o^2} + \frac{k_2}{2}(x^2 - y^2)
\end{equation}

Note that the resulting Hamiltonian has no mixed terms, so the equations are naturally separated into the different dimensions. This is in part due to our selection of external field, if we selected a skew quadrupole the transverse dimensions would be coupled. We can see that any static multipole field will not result in any coupling to the longitudinal dimensions in the paraxial approximation. For the quadrupole the most relevant effect is the effective field for different momentum, the transverse "Chromaticity" in analogy to light optics. This is properly treated at higher orders, but we will first resolve this to lowest order. Applying Hamilton's equations of motion gives us three systems of linear differential equations.

\begin{equation} \label{eq:quad_diffeq}
\begin{split}
	\frac{dx}{ds} &= p_x\\ 
	\frac{dp_x}{ds} &= -k_2 x
\end{split}
\end{equation}

Substituting and rearranging we arrive at Hills equation, Eq. \ref{eq:hills}

\begin{equation} \label{eq:hills}
	\frac{d^2 x}{ds^2} + k_2 x = 0
\end{equation}

Which has three general solutions for different ranges of $k_2$.

\begin{align} \label{eq:hillSol}
	x(s) &= a \cos{\left(\sqrt{k_2} s\right)} + b \sin{\left(\sqrt{k_2} s\right)}, &k_2>0\\
	x(s) &= a s + b, &k_2 =0 \\
	x(s) &= a \cosh{\left(\sqrt{k_2} s\right)} + b \sinh{\left(\sqrt{k_2} s\right)}, &k_2<0
\end{align}

For a quadrupole of a given length $L$, we can define a matrix Eq. \ref{mat:quad} which represents the three linear solutions for our different dimensions if it acts on a state vector of our canonical coordinates.

\begin{multline} \label{mat:quad}
	M_{quad} = \\
\begin{pmatrix} 
	&\cos{\left(\sqrt{k_2} L\right)} &\frac{1}{\sqrt{k_2}}\sin{\left(\sqrt{k_2} L\right)} &0 &0 &0 &0\\
	&-\sqrt{k_2}\sin{\left(\sqrt{k_2} L\right)} &\cos{\left(\sqrt{k_2} L\right)} &0 &0 &0 &0\\
	&0 &0 &\cosh{\left(\sqrt{k_2} L\right)} &\frac{1}{\sqrt{k_2}}\sinh{\left(\sqrt{k_2} L\right)} &0 &0\\
	&0 &0 &\sqrt{k_2}\sinh{\left(\sqrt{k_2} L\right)} &\cosh{\left(\sqrt{k_2} L\right)} &0 &0\\
	&0 &0 &0 &0 &1 &\frac{L}{\beta_o^2\gamma_o^2}\\
	&0 &0 &0 &0 &0 &1\\
\end{pmatrix}
\end{multline}

The same approach can be applied to our other separated function, first order elements in the paraxial approximation, for a drift space (Eq. \ref{mat:drift}) and a sector dipole magnet (Eq. \ref{mat:dipole}).

\begin{equation} \label{mat:drift}
	M_{drift} =
\begin{pmatrix}
	&1 &L &0 &0 &0 &0\\
	&0 &1 &0 &0 &0 &0\\
	&0 &0 &1 &L &0 &0\\
	&0 &0 &0 &1 &0 &0\\
	&0 &0 &0 &0 &1 &\frac{L}{\beta_o^2\gamma_o^2}\\
	&0 &0 &0 &0 &0 &1\\
\end{pmatrix}
\end{equation}

\begin{multline} \label{mat:dipole}
	M_{dipole} = \\
\begin{pmatrix}
	&\cos{\left(\frac{L}{\rho}\right)} &\rho \sin{\left(\frac{L}{\rho}\right)} &0 &0 &0 &-\frac{\rho}{\beta_o}\left(1 - \cos{\left(\frac{L}{\rho}\right)}\right)\\
	&-\frac{1}{\rho}\sin{\left(\frac{L}{\rho}\right)} &\cos{\left(\frac{L}{\rho}\right)} &0 &0 &0 &-\frac{1}{\beta_o}\sin{\left(\frac{L}{\rho}\right)}\\
	&0 &0 &1 &L &0 &0\\
	&0 &0 &0 &1 &0 &0\\
	&\frac{1}{\beta_o}\sin{\left(\frac{L}{\rho}\right)} &\frac{\rho}{\beta_o}\left(1 - \cos{\left(\frac{L}{\rho}\right)}\right) &0 &0 &1 &\frac{\rho}{\beta_o^2}\sin{\left(\frac{L}{\rho}\right)} - L\\
	&0 &0 &0 &0 &0 &1\\
\end{pmatrix}
\end{multline}

Note that the dipole preserves coupling between the longitudinal and horizontal planes at the first order, unlike the quadrupole. This gives rise to first order dispersion, or longitudinally dependent energy coupling in the horizontal motion. 

These solutions are quite powerful, as they let us stack linear transformations in a row to construct mathematical representations of a transfer line. There is an additional advantage to this form, since the equations of motion were derived from the Hamiltonian of our system, we are guaranteed that these transformations are symplectic. Formally a "symplectic form is a two-form which is closed and nondegenerate" \cite{joseClassicalDynamicsContemporary1998}. More usefully in a physics context a symplectic transformation is one which is guaranteed to preserve our physical quantities, such as total energy. A sequence of symplectic mappings is also symplectic, so we can be certain that any sequence of our above matrix elements will maintain the symplectic condition. Symplecticity is a rich topic in the context of dynamical systems, but beyond the scope of this thesis, the author recommends the following references for further edification \cite{forestGeometricIntegrationParticle2006,yoshidaConstructionHigherOrder1990}.

\section{Courant-Snyder Parameterization} \label{sec:CSparam}
While the transformations we have acquired are generally applicable, we would like to evaluate the conditions for a stable periodic system like we need for a cyclic machine. At this point we will make another simplification of the dynamics by investigating the stability of the transverse and longitudinal dimensions separately. This is of course not strictly true, but the characteristic timescales of synchrotron oscillations are often orders of magnitude slower than the transverse oscillations. We will choose the horizontal "x" motion to investigate. Analogous methods can be used for the "y" plane.

We begin with defining a matrix of a periodic "cell" (Eq. \ref{eq:period}), or sequence of elements $M_1 \dots M_n$ with total length $L$. This could be the representation of a whole cyclic accelerator, or smaller sequence of elements intended for a superperiodic construction.

\begin{equation} \label{eq:period}
	\mathbb{M}(s) = M(s+L|s) = M_nM_{n-1}\dots M_2M_1
\end{equation}

Our periodic condition then becomes that motion is bounded for repeated applications of this matrix. The solutions to Hills equations (including the dipole case where longitudinal coupling is excluded) yields matrices with a determinant of one. As the determinant of a product of matrices is the product of their individual determinants, we know that the determinant of our particular cell must be 1. If we decompose $\mathbb{M}$ into eigenvalues $\lambda_1, \lambda_2$ and eigenvectors $\vec{v}_1, \vec{v}_2$, the unit determinant leads to two useful relationships. First $\lambda_1 + \lambda_2 = \mathrm{Trace}(\mathbb{M})$, and second $\lambda_1 = 1/\lambda_2$. Substituting and rearranging yields Eq. \ref{eq:eigenTrace}.

\begin{equation} \label{eq:eigenTrace}
	\lambda_1^2 - \mathrm{Trace}(\mathbb{M})\lambda_1 + 1 = 0
\end{equation}

If we describe a canonical state vector as a linear combination of our eigenvectors we can get a straightforward requirement on our eigenvalues with repeated application of the transfer map (Eq. \ref{eq:eigenRep}).

\begin{equation} \label{eq:eigenRep}
	\vec{q}_n = \mathbb{M}^n\vec{q}_o = \mathbb{M}^n(a \vec{v}_1 + b \vec{v}_2) = a \lambda_1^n \vec{v}_1 + b \lambda_2^n \vec{v}_2
\end{equation}

This relationship clearly demonstrates that for stable long term motion, these eigenvalues cannot grow with repeated iterations. Courant and Snyder proposed the following general parameterization of our transfer matrix in Eq. \ref{eq:csMat}.

\begin{equation} \label{eq:csMat}
	\mathbb{M}_{CS} = 
\begin{pmatrix}
&\cos{\left(\phi_x\right)} + \alpha_x\sin{\left(\phi_x\right)} &\beta_x\sin{\left(\phi_x\right)}\\
&-\gamma_x\sin{\left(\phi_x\right)} &\cos{\left(\phi_x\right)} - \alpha_x\sin{\left(\phi_x\right)}\end{pmatrix}
\end{equation}

If we calculate the trace of this parameterization and substitute into Eq. \ref{eq:eigenTrace}, we find values of the eigenvalues to be Eq. \ref{eq:eigenSol}

\begin{equation} \label{eq:eigenSol}
\begin{split}
	\lambda_1 &= e^{i\phi_x}\\
	\lambda_2 &= e^{-i\phi_x}\\
\end{split}
\end{equation}

Where $\phi_x$ is complex if $|\mathrm{Trace}(\mathbb{M})| > 2$ and real if $|\mathrm{Trace}(\mathbb{M})| < 2$. We can see from Eq. \ref{eq:eigenRep}, our motion is only bounded for real values of $\phi_x$ and our stability condition for a periodic transfer matrix $\mathbb{M}$ becomes $|\mathrm{Trace}(\mathbb{M})| < 2$.

By leveraging the fact that the periodic linearized equations of motion have a determinant of 1, we can arrive at the relationship in Eq. \ref{eq:csRel}. We can also see from the Courant-Snyder parameterization Eq. \ref{eq:phiTrace} holds for any stable transfer matrix. This defines the fractional component of $\phi_x$, but leaves it ambiguous to integer multiples of $2\pi$. In practice, the ambiguity can be cleared up by calculating this parameter for individual constituents of the periodic transfer matrices.

\begin{equation} \label{eq:csRel}
	\beta_x \gamma_x  = 1 + \alpha_x^2
\end{equation}

\begin{equation} \label{eq:phiTrace}
	\cos{\left(\phi_x\right)} = \mathrm{Trace}(\mathbb{M})
\end{equation}

We can further investigate the form of these parameters by splitting the general periodic matrix, in Eq. \ref{eq:csMatSplit}, where the matrix $A$ and $S$ are given in Eq. \ref{mat:csA}. $S$ is a block antisymmetric matrix, and reduces to just the antisymmetric matrix in two dimensions like we have here.

\begin{equation} \label{eq:csMatSplit}
	\mathbb{M}_{CS} = I \cos{\left(\phi_x\right)} + S A \sin{\left(\phi_x\right)}
\end{equation}

\begin{equation} \label{mat:csA}
	A = 
\begin{pmatrix}
	&\gamma_x &\alpha_x\\
	&\alpha_x &\beta_x
\end{pmatrix},
\hspace{10pt} S =
\begin{pmatrix}
	&0 &1\\
	&-1 &0
\end{pmatrix}
\end{equation}

Within our periodic cell $\mathbb{M}$, we can consider the transformation in Eq. \ref{eq:startShift} by some transfer matrix in the cell which moves the overall start point. By examining the trace of this transformation, we can see that $\phi_x$ of our periodic matrix is unchanged by this shift in start point.

\begin{equation} \label{eq:startShift}
	\mathbb{M}(s_1)  = M(s_1|s_o)\mathbb{M}(s_o)M(s_1|s_o)^{-1}
\end{equation}

If we apply this transformation to \ref{eq:csMatSplit}, as $\phi_x$ unadjusted by the shift, the overall effect is summarized in Eq. \ref{eq:AstartShift}.

\begin{equation} \label{eq:AstartShift}
	S A(s_1) = M(s_1|s_o) S A(s_o)M(s_1|s_o)^{-1}
\end{equation}

We can now leverage the underlying symplecticity of our transfer matrices, which mandates the condition in Eq. \ref{eq:symplecticMat}.

\begin{equation} \label{eq:symplecticMat}
	M^T S M = S
\end{equation}

Recognizing that $S^T = S^{-1}$ and using the symplectic condition, we can rearrange and invert Eq. \ref{eq:AstartShift} to arrive at the transformation of $A^{-1}$ at different points in the cell in Eq. \ref{eq:barAshift} 

\begin{equation} \label{eq:barAshift}
	A(s_1)^{-1} = M(s_1|s_o)  A(s_o)^{-1} M(s_1|s_o)^T
\end{equation}

After inverting $A$ we can solve Eq. \ref{eq:barAshift} for the transformation of our Courant-Snyder parameters $\beta_x, \alpha_x, \gamma_x$ with respect to the components of the transfer matrix $M$ in Eq. \ref{eq:csShift}.

\begin{equation} \label{eq:csShift}
	\begin{pmatrix} &\beta_x(s_1) \\ &\alpha_x(s_1)\\ &\gamma_x(s_1) \end{pmatrix} = 
	\begin{pmatrix}
		&M_{11}^2 &-2 M_{11} M_{12} &M_{12}^2\\
		&-M_{11} M_{21} &M_{11} M_{22} + M_{12} M_{21} &-M_{12}M_{22}\\
		&M_{21}^2 &-2 M_{21} M_{22} &M_{22}^2
	\end{pmatrix}
	\begin{pmatrix} &\beta_x(s_o) \\ &\alpha_x(s_o)\\ &\gamma_x(s_o) \end{pmatrix}
\end{equation}

This is a useful result, as it means for any stable, periodic, sequence of our linear beamline elements, we can uniquely define the Courant-Snyder parameters and transfer them to any other point in the lattice. The result in \ref{eq:csShift} is even useful without a periodic system, as the effective lattice parameters of a beam distribution (described later in this section) can be transformed along a sequence of linear elements as is the case in a linac or transfer line.

We would now like to find the equations of motion for this parameterization. We can construct an invariant action for our canonical coordinates and Courant-Snyder parameters.

\begin{equation} \label{eq:csJ}
	J = \frac{1}{2} (\vec{q}_x)^T A \vec{q}_x = \frac{1}{2} (\gamma_x x^2 + 2\alpha_x x p_x + \beta_x p_x^2)
\end{equation}

Which we verify is invariant by inspecting the transformation of our canonical state vectors $\vec{q}_X$ and matrix $A$ under a transfer matrix.

\begin{equation} \label{eq:Jinvariant}
	J(s_1) = \frac{1}{2} \vec{q}_x(s_1)^T A(s_1) \vec{q}_x(s1) = \frac{1}{2} \vec{q}_x(s_o)^T M^T (M^T)^{-1} A(s_o) M^{-1}  M \vec{q}_x(s_o) = J(s_o)
\end{equation}

Determining the angle variable with respect to the Cournant-Snyder parameters is an extended derivation which requires inspecting the general solution to Hill's equation using the Floquet transform in comparison with the current periodic condition. This would be longer than the entire derivation of the Courant-Snyder parameters up to this point, so it is omitted here and only the important results are quoted. First the relationship to the angle variable $\varphi_x$ to the coordinates.

\begin{equation} \label{eq:csAngle}
	\tan{(\varphi_x)} = -\beta_x \frac{x}{p_x} - \alpha_x 
\end{equation}

Second, the relationship to the $\beta_x$ parameter, to the focusing term $K(s)$ in Eq. \ref{eq:floqetBet}. Here $K(s)$ is a stand in for all of our Hill's equation conditions, both focusing and defocusing and the weak focusing effect from dipoles. 

\begin{equation} \label{eq:floqetBet}
	\begin{split}
	&\frac{d^2}{ds^2}\sqrt{\beta_x(s)} + K(s)\sqrt{\beta_x(s)} = \frac{1}{\sqrt{\beta_x^3(s)}} \hspace{5pt} or,\\ 
	&\frac{\beta_x(s)}{2}\frac{d^2\beta_x(s)}{ds^2} - \frac{1}{4}\left(\frac{d\beta_x(s)}{ds}\right)^2 + K(s)\beta_x^2(s) = 1
	\end{split}
\end{equation}

Third we can recognize an additional relationship between the $\alpha_x$ and $\beta_x$ functions in Eq. \ref{eq:alphaToBet}

\begin{equation} \label{eq:alphaToBet}
	\alpha_x(s) = -\frac{1}{2}\frac{d\beta_x(s)}{ds}
\end{equation}

Finally we can relate the angle variable to the $\beta_x$ function (Eq. \ref{eq:varphiBet}) and our phase advance $\phi_x$ (Eq. \ref{eq:phiBet}.

\begin{equation} \label{eq:varphiBet}
	\frac{d\varphi_x(s)}{ds} = \frac{1}{\beta_x(s)}
\end{equation}

\begin{equation} \label{eq:phiBet}
	\phi_x(s_1|s_o) = \varphi(s_1) - \varphi(s_o) \Rightarrow \phi_x(s_1|s_o) = \int_{s_o}^{s_1} \frac{1}{\beta_x(s)} ds
\end{equation}

The result is that we only need the function $\beta_x(s)$ and its derivative to fully parameterize the linear motion of our single particles in an accelerator. We can further expand on this by finding the equations of motion with respect to this phase angle $\varphi_x$ by inverting Equations \ref{eq:csAngle} and \ref{eq:Jinvariant} to arrive Equations \ref{eq:csXvarphi} and \ref{eq:csPvarphi}.

\begin{equation} \label{eq:csXvarphi}
	x = \sqrt{2 \beta_x J_x} \cos{(\varphi_x)}
\end{equation}

\begin{equation} \label{eq:csPvarphi}
	p_x = -\sqrt{\frac{2 J_x}{\beta_x}} (\sin{(\varphi_x)} + \alpha_x \cos{(\varphi_x)})
\end{equation}

We can see that the combination of these equations of motion paramaterize a ellipse for our Poincare section at any given point along the beamline. Figure \ref{fig:csEllipse} shows such an ellipse with the relevant relationship to the Courant-Snyder parameters. This graphic makes clear some useful intuitive relationships of the paramaterization. We can see that the $\beta_x$ describes the transverse envelope of motion for the lattice at a given location, and is defined to be strictly positive. The $\gamma_x$ function must also be strictly positive and parameterizes something of a "momentum envelope", though this is much less useful than the amplitude. The phase advance $\phi_x$ can be seen as sort of betatron oscillation time parameter. Regions with small beta functions, or small beam envelopes have large phase advances and represent large advances in the betatron oscillations.

The area of this ellipse is $2\pi J_x$, but is usually related to the "emittance" in Eq. \ref{eq:csEmit} as $\pi \epsilon_x$. There is some inconsistency in application of the emittance in accelerator literature, sometimes it is only considered to be an aggregate property of a particle bunch and other times it may relate to a single particle equivalent dynamical value. I will distinguish the aggregate property as e.g. $\epsilon_{x,rms}$, defined in section \ref{sec:bunchEmit}, to distinguish it from the dynamical value $\epsilon_x$.

\begin{equation} \label{eq:csEmit}
	\epsilon_x = 2 J_x
\end{equation}

\begin{figure} 	\centering
	\includegraphics[width=\linewidth]{phase_space_ellipseImpactX.pdf}
	\caption{Poincare section ellipse of linear betatron motion with associated relationships to Courant-Snyder parameters \cite{hueblNextGenerationComputational2022}}
	\label{fig:csEllipse}
\end{figure}

Based on these result we can introduce the "normalized" (they still have dimension of $\sqrt{m}$) Cournat-Snyder parameters in \ref{eq:csNorm}.

\begin{equation} \label{eq:csNorm}
\begin{split}
	x_N &= \frac{x}{\sqrt{\beta_x}}\\
	p_{xN} &= \frac{\alpha_x x}{\sqrt{\beta_x}} - \sqrt{\beta_x}p_x
\end{split}
\end{equation}

This normalization reduces the equations of motion to that of circles with radius equal to the square root of the emittance. These coordinates are also sometimes referred to as Floquet coordinates in accelerator literature.

\begin{equation} \label{eq:csNormVarphi}
\begin{split}
	x_N &= \sqrt{\epsilon_x} \cos{(\varphi_x)}\\
	p_{xN} &= -\sqrt{\epsilon_x} \sin{(\varphi_x)}
\end{split}
\end{equation}

We can also observe that the definition of the emittance simplifies in these coordinates (Eq. \ref{eq:csNormEmit}), and our linearized Hamiltonian reduces to Eq. \ref{eq:csNormHam}.

\begin{equation} \label{eq:csNormEmit}
	\epsilon_x = p_{xN}^2 + x_{N}^2
\end{equation}

\begin{equation} \label{eq:csNormHam}
	H = \frac{p_{xN}^2}{2\beta_x} + \frac{x_{N}^2}{2\beta_x}
\end{equation}

There is one more important dynamical quantity that needs to be defined. The betatron tune $Q_x$ is the total phase advance over the full periodicity of an accelerator, defined in Eq. \ref{eq:csTune} where $C$ is the overall machine circumference. While some machines may be constructed of periodic cells, the overall tune is the fundamental periodic condition and necessary to understand the overall machine's stability.

\begin{equation} \label{eq:csTune}
	Q_x = \int_{s_o}^{s_o + C} \frac{1}{\beta_x(s)} ds
\end{equation}

All of this parameterization was of course in one dimension, the same can be applied to the y-plane assuming they are uncoupled. An extended parameterization to include coupling has been considered and there are two main approaches. One by Edwards-Teng \cite{edwardsParametrizationLinearCoupled1973} and one by Mais-Ripkin \cite{borchardtCalculationBeamEnvelopes1988}. A good overview including many extenisions to these parameterizations is provided in \cite{vanweldeReviewCoupledBetatron2022}. For the purposes of this thesis these coupled parameterizations are extraneous and only affect the tune match possible which will be discussed in chapter \ref{chap:experiment} on experimental design.

At this point I would like to reiterate the assumptions along the way, and comment on the impacts.
\begin{enumerate}
	\item The equations of motion are for single particles in external electromagnetic field - interaction with the environment and other particles due to the particles self fields are not considered
	\item The motion is defined with respect to a reference particle traveling with a design momentum along a desired design trajectory - requires definition of this reference
	\item The confining potentials are hard edged constant magnetic fields - Impacts of realistic magnetic field fall-off are assumed to be small
	\item The transverse momentum is assumed to be small compared to the longitudinal momentum, which lets us linearize with the paraxial approximation - approach is valid only for situations with sufficiently high relativistic $\beta_o$ factors
	\item The transverse "betatron" oscillations are assumed to be much faster than the longitudinal "synchrotron" oscillations and the Courant-Snyder parameterization is for particles with the design momentum - energy dependent effects are not considered
\end{enumerate}

With these caveats in mind, this approach has been very powerful and is the guiding principle for first order accelerator design since its introduction. There is a wide variety of lattice simulation codes dating back to the 60's specifically designed to calculate these lattice parameters for evaluating stability, steering, and aperture restrictions of linear lattices.

\section{First Order Bunch Properties} \label{sec:bunchEmit}
In practice we rarely accelerate single particles, and so we would like to parameterize the collective properties of the particle bunch. ef we have some bunch of particles all undergoing betatron oscillations we can relate their individual oscillations to the ensemble average or first moment in Eq. \ref{eq:bunchMean}.

\begin{equation} \label{eq:bunchMean}
	\langle x \rangle  = \sqrt{\beta_x} \langle \sqrt{\epsilon_x} \cos{(\varphi_x)}\rangle 
\end{equation}

Here we can extract the beta function since we know it is a longitudinally dependent parameter of the lattice and not our individual particles. For a typical bunch we can assume a uniform distribution of oscillation phases, and we know from our equations of motion that the emittance and betatron phase are uncorrelated, so we can see that this mean is typically zero. This form can be exploited by adding a coherent correlated emittance and phase term to the bunch with a "Kicker" element. This adds a coherent term to Eq. \ref{eq:bunchMean} and we can observe the bunch centroid evolving according to the lattice dynamics. If we look at the second moments we can extract more information on the bunch distribution. We start with the transverse coordinate, Eq. \ref{eq:momX2}.

\begin{equation} \label{eq:momX2}
	\langle x^2 \rangle = \beta_x \langle \epsilon_x \cos{(\varphi_x)}^2 \rangle
\end{equation}

If we evaluate again for a uniform distribution of the betatron phase, we arrive at Eq. \ref{eq:emitDef}

\begin{equation} \label{eq:emitDef}
	\langle x^2 \rangle = \frac{\beta_x}{2} \langle \epsilon_x \rangle
\end{equation}

We then define our rms emittance in Eq. \ref{eq:rmsEmit}. 

\begin{equation} \label{eq:rmsEmit}
	\epsilon_{x,rms} = \frac{1}{2} \langle \epsilon \rangle
\end{equation}

Using the same approach, we can relate the emittance to the two other moments of the distribution.

\begin{equation} \label{eq:momP2}
\begin{split}
	\langle x p_x \rangle &= -\alpha_x \epsilon_{x,rms}\\
	\langle p_x^2 \rangle &= \gamma_x \epsilon_{x,rms}
\end{split}
\end{equation}

Based on the relationship between the Courant-Snyder parameters in Eq. \ref{eq:emitMoment}.

\begin{equation} \label{eq:emitMoment}
	\epsilon_{x,rms} = \sqrt{\langle x^2 \rangle \langle p_x^2 \rangle - \langle x p_x \rangle^2}
\end{equation}

These values can be directly related to measurable quantities. This of course requires knowledge of the beam distribution, but the core of most beams are Gaussian and Eq. \ref{eq:rmsMoment} is generally useful.

\begin{equation} \label{eq:rmsMoment}
	\sigma_x = \sqrt{\beta_x \epsilon_{x,rms}}
\end{equation}

\section{Nonlinear Perturbations} \label{sec:nonlinearPerturb}
Our treatment of the dynamics thus far has been exclusively linear. In practical machines, higher order multipoles are often added to compensate for deficiencies in the linear lattice construction. In most modern circular accelerators the transverse chromaticity is compensated, or at least controlled with sextupole magnets. Octupole magnets may also be added to add an amplitude dependent detuning, which in turn drives tune spread in the bunch and can provide landau damping. In addition to deliberately introduced nonlinear perturbations, our linear elements have small intrinsic nonlinearities. Most notably there is an exact, nonlinear mapping for the dipole which has higher order terms which become relevant for large bending element \cite{bruhwilerSymplecticPropagationMap}s. Additionally, the fringe fields of linear elements will possess intrinsic nonlinearities, though this is strongly dependent on the design \cite{baartmanQuadrupoleShapes2012}. For example, the quadrupole fringe field in the hard-edge approximation has contributions of the same order as an octupole magnet \cite{forestLeadingOrderHard1988}. These contributions all serve to perturb the dynamics, we can briefly consider the next order case of a sextupole magnet. If we add the sextupole term to our general Hamiltonian in Eq. \ref{eq:H_As}, and take the case for the curvature to be zero we arrive at Eq. \ref{eq:H_sext}.

\begin{equation} \label{eq:H_sext}
	H = -\sqrt{1 + p_t^2 - \frac{2p_t}{\beta_o} - p_x^2 - p_y^2} - \frac{p_t}{\beta_o} + \frac{k_3}{3}(3 x y^2 - x^3)
\end{equation}

Even in the linearized case, we cannot find an analytic solution for the equations of motion for this system. There are a number of analytical techniques for analyzing the motion of these nonlinear systems, with Lie algebra approaches being the most common, but this is beyond the scope of this thesis. We can instead consider a quick numerical example using a thin mapping of the integrated effect of the sextupole term in Eq \ref{eq:sextKick}, analogous to our matrix treatment of the linear dynamics. For illustrative purposes, we will continue to restrict ourselves to the horizontal plane.

\begin{equation} \label{eq:sextKick}
	\Delta p_x = Lk_3x^2
\end{equation}

%Figure \ref{fig:sextMap} shows the Poincare map of a simple 1D periodic system with a tune of 0.3$\pi$ and a single thin sextupole kick like above. 

Figure \ref{fig:sextMap} shows the Poincare map for beam measurements in the Indiana University Cooler Synchrotron with sextupole effects. It demonstrates two important effects of the sextupole perturbation, there is an amplitude limit to stable motion, referred to as the dynamic aperture in accelerator literature. It also shows significant deviation of the trajectories from the linear elliptical traces, indicating that the emittance is not conserved.

\begin{figure} 
	\centering
	\includegraphics[width=0.8\linewidth]{iucfSextupole.png}
	\caption{Phase space in canonical and action angle coordinates for linear system with strong sextupole perturbations \cite{caussynExperimentalBeamDynamics}}
	\label{fig:sextMap}
\end{figure}

\section{Nonlinear Resonances} \label{sec:resonances}
There is an additional effect of nonlinear perturbations to the linear dynamics, they serve to drive additional resonances. The general condition for a resonance is given in Eq \ref{eq:resonance}, where $n$,$m$, and $k$ are integers.

\begin{equation} \label{eq:resonance}
	nQ_x + mQ_y = k\pi
\end{equation}

At these characteristic frequencies contributions add coherently and drive unstable motion. The order of resonance is determined by the sum of $n$ and $m$. If $n + m = 3$ this is a third integer resonance and can be driven by sextupoles. Resonances are not limited to nonlinear driving components, quadrupoles drive half integer resonances, all multipole terms drive integer resonances.The basic resonance condition results in the dense map of resonance lines up to the order 6 in tune space in Figure \ref{fig:resonanceLines}. 

\begin{figure} 
	\centering
	\includegraphics[width=0.8\linewidth]{order6resonance.pdf}
	\caption{Resonance lines in tune space up to the 6th order}
	\label{fig:resonanceLines}
\end{figure}


Not every resonance is dangerous, only those which are driven strongly by the particular lattice configuration cause instabilites. Carefully picking the working point, or central tune of a lattice is an important design consideration.

\section{Integrable Systems} \label{sec:integral}
We have seen that uncontrolled nonlinear perturbations can significantly reduce the stability of our system. An integrable system is one which has as many independent conserved quantities as degrees of freedom in the configuration space. This serves to regularize the motion and means there cannot be chaotic trajectories, though it does not guarantee that trajectories must be bounded. Additionally the KAM theorem ensures that most trajectories away from resonances of a perturbed integrable system remain stable and quasi-periodic \cite{moserSolarSystemStable1978}. This is an important result, as it means a realistic, flawed implementation of an integrable system will still demonstrate broad stability. The predictability of integrable systems means they have long been a subject of study in dynamics. Many forms of integrable systems have been found \cite{hietarintaDirectMethodsSearch1987}, though they generally are physically impossible to impractical to implement. Particle accelerators are one of the few areas where careful study of the controlled dynamical systems can take place.

The linear Courant-Snyder system is an integrable system, with an independently conserved emittance in each degree of freedom. This also holds in the coupled linear system, thought the parameterization is more complicated. In the context of accelerator dynamics, the advantages of this integrability are clear. It indicates that the design orbits are stable, generally robust to small permutations, and can be parameterized from the conserved quantities. The drawbacks of perturbative nonlinearites introduced to the linear system to mitigate higher order effects, have motivated a long history of searches for practical nonlinear integrable accelerators. McMillan introduced a basic set of one dimensional thin lens solutions \cite{mcmillanThoughtsStabilityNonlinear1967}, and Danilov found a practical 1-D solution using the Beam-Beam force as the nonlinear confining potential \cite{danilovTwoExamplesIntegrable1997}. The experiments in this thesis focus on another development of Danilov and Nagaitsev in 2010.

\section{Nonlinear Integrable Optics} \label{sec:nio}
Danilov and Nagaitsev discovered a two dimensional nonlinear integrable system relevant for accelerators \cite{danilovNonlinearAcceleratorLattices2010}. To derive this system, we begin with a Hamiltonian in our transverse space in our Courant-Snyder normalized parameters, so there is no time dependence of the Hamiltonian.  If we add a general nonlinear potential constructed out of these normalized coordinates, it too will be time independent and the Hamiltonian will be a constant of motion, as in Eq. \ref{eq:H_norm}.

\begin{equation} \label{eq:H_norm}
	H = \frac{p_{xN}^2}{2} + \frac{p_{yN}^2}{2}  + \frac{x_{N}^2}{2} + \frac{y_{N}^2}{2} + V(x_N,y_N,s)
\end{equation}

We can make the form of $V(x_N,y_N,s)$ more straightforward to define in lab coordinates if we elect to place the nonlinear potential in a position with matched horizontal and vertical beta functions. The only way to accomplish this is in a drift space. In order to arrive at this condition, we can define a periodic cell consisting of a matched drift and a so called "t-insert" which evaluates to a matrix with equal focusing in both planes, dependent on the desired phase advance in the drift. We can construct this matching lattice out of whatever elements are convenient. 

The general problem is then to find a potential which can generate a second invariant of motion $I$. We absorb the linear potential terms into an overall potential to yield Eq. \ref{eq:H_U}. 

\begin{equation} \label{eq:H_U}
	H = \frac{p_{xN}^2}{2} + \frac{p_{yN}^2}{2} + U(x_N,y_N)
\end{equation}

The approach used by Danilov and Nagaitsev looked for forms of $I$ quadratic in the normalized momentum. Three solutions were found, though two were determined to have practical issues impeding physical implementation. Most notably, the phase space trajectories could not circle the origin, and would require some sort of "C"-shaped vacuum chamber. The choice of constants leading to the first solution yields Eq. \ref{eq:darbroux} as the constraint on $U$ for a second integral.

\begin{equation} \label{eq:darbroux}
	x_N y_N\left( \frac{\partial^2 U}{\partial x_N^2} - \frac{\partial^2 U}{\partial y_N^2}\right) + (y_n^2 - x_n^2 + c^2)\frac{\partial^2 U}{\partial x_N y_N} + 3y_N \frac{\partial U}{\partial x_N} - 3 x_N \frac{\partial U}{\partial x_N} = 0
\end{equation}

This is Darbroux's equation \cite{darbouxProblemeMecanique1901}, and has the general solution of Eq. \ref{eq:darbrouxSol}, where $\xi$ and $\eta$ are elliptical coordinates given in Eq. \ref{eq:ellipCoords} and $f(\xi)$ and $g(\eta)$ are arbitrary functions. Note that there is a constant $c$ introduced with units of the square root of length.

\begin{equation} \label{eq:darbrouxSol}
	U(x_N,y_N) = \frac{f(\xi) + g(\eta)}{\xi^2 - \eta^2}
\end{equation}

\begin{equation} \label{eq:ellipCoords}
\begin{split}
	\xi &= \frac{1}{2c}\left[ \sqrt{(x_N+c)^2 + y_N^2} + \sqrt{(x_N-c)^2 + y_N^2} \right] \\
	\eta &= \frac{1}{2c}\left[ \sqrt{(x_N+c)^2 + y_N^2} - \sqrt{(x_N-c)^2 + y_N^2} \right]
\end{split}
\end{equation}

These can be related back to our transverse coordinates with the relationships in Eq. \ref{eq:ellipNorm}

\begin{equation} \label{eq:ellipNorm}
\begin{split}
	x_N &= c\xi\eta \\
	y_N &= c \sqrt{(\xi^2-1)(1-\eta^2)}
\end{split}
\end{equation}

These functions result in a second invariant of motion of the form in Eq. \ref{eq:I_fg}

\begin{equation} \label{eq:I_fg}
	I = (x_N p_{yN} - y_N p_{xN})^2 + c^2 p_{xN}^2 + 2c^2 \frac{f(\xi)\eta^2 + g(\eta)\xi^2}{\xi^2 - \eta^2}
\end{equation}

The particular form of the $f(\xi)$ and $g(\eta)$ functions were selected to also satisfy Laplace's equation so the potential $U$ could be generated by static magnetic fields. The first solution to consider is the linear potential Eq. \ref{eq:linFG}, which we recast into this form in Eq. \ref{eq:linEllipse}

\begin{equation} \label{eq:linFG}
	\frac{f_1(\xi) + g_1(\eta)}{\xi^2 - \eta^2} = \frac{x_N^2}{2} + \frac{y_N^2}{2}
\end{equation}

\begin{equation} \label{eq:linEllipse}
\begin{split}
	f_1(\xi) &= \frac{c^2}{2}\xi^2(\xi^2-1)\\
	g_1(\eta) &= \frac{c^2}{2}\eta^2(1 - \eta^2)
\end{split}
\end{equation}

The more difficult case is to find a nonlinear solution to these constraints. This was found to be the functions in \ref{eq:nonFG}. Here $b$, $d$ and $\tilde{t}$ are arbitrary constants with units of length.

\begin{equation} \label{eq:nonFG}
\begin{split}
	f_2(\xi) &= \xi \sqrt{\xi^2 -1}(d + \tilde{t} \hspace{2pt} \textrm{arccosh}(\xi)) \\
	g_2(\eta) &= \eta \sqrt{1-\eta^2}(b + \tilde{t} \hspace{2pt} \textrm{arccos}(\eta))
\end{split}
\end{equation}

Inspection of the form of the nonlinear potential functions reveals that selecting $d=0$ and $b=-\tilde{t}\frac{\pi}{2}$ results in a magnetic field with not dipole term in the multipole expansion and therefore no bending in the nonlinear region. We also introduce a unitless nonlinear scaling parameter $t$ such that $\tilde{t} = tc^2$. Making all of these substitutions we arrive at a form of these potentials in Eq. \ref{eq:nonFgNorm}.

\begin{equation} \label{eq:nonFgNorm}
\begin{split}
	f_2(\xi) &= t c^2 \xi \sqrt{\xi^2 -1} \textrm{arccosh}(\xi) \\
	g_2(\eta) = t c^2 \eta \sqrt{1-\eta^2}&\left(\arccos{(\eta) - \frac{\pi}{2}}\right) = -t c^2 \eta \sqrt{1-\eta^2}\arcsin{(\eta)}
\end{split}
\end{equation}

We can then consider the linear combination of these individual solutions, i.e. $f(\xi) = f_1(\xi)+f_2(\xi)$, and after substitution and simplification, we arrive at the form of the equations in Equations \ref{eq:H_DN} and \ref{eq:I_DN}.

\begin{multline} \label{eq:H_DN}
	H_{DN} = \frac{p_{xN}^2}{2} + \frac{p_{yN}^2}{2}  + \frac{x_{N}^2}{2} + \frac{y_{N}^2}{2} +\\
	tc^2 \frac{\xi \sqrt{\xi^2 -1} \textrm{arccosh}(\xi) - \eta \sqrt{1-\eta^2}\arcsin{(\eta)}}{\xi^2 - \eta^2} 
\end{multline}

\begin{multline} \label{eq:I_DN}
	I_{DN} = (x_N p_{yN} - y_N p_{xN})^2 + c^2 p_{xN}^2 + c^2 x_N^2 +\\ 
	2t c^4 \xi \eta \frac{\eta\sqrt{\xi^2 -1} \textrm{arccosh}(\xi) - \xi \sqrt{1-\eta^2}\arcsin{(\eta)}}{\xi^2 - \eta^2}
\end{multline}

This parameterization is presented as it was the form used in the original derivation, and most clearly arises from the underlying partial differential equations. However it is quite cumbersome, and encounters issues in numerical tracking for small values of $y_N$. A separate parameterization in complex coordinates was proposed in \cite{mitchellComplexRepresentationPotentials2019}, and will be used for the rest of this thesis. To begin with, the coordinates in this representation are scaled by the geometric parameter $c$ to be fully unitless, and a complex coordinate $z$ is introduced, Eq. \ref{eq:CMcoord}.

\begin{equation} \label{eq:CMcoord}
\begin{split}
	x_c &= \frac{x_N}{c} = \frac{x}{c\sqrt{\beta(s)}}\\
	p_{xc} = \frac{p_{xN}}{c} &= p_x\frac{\sqrt{\beta(s)}}{c} + \frac{\alpha(s)x}{c\sqrt{\beta(s)}}\\
	z_c &= x_c + iy_c
\end{split}
\end{equation}

The resulting Hamiltonian and second integral of motion are given in Equations \ref{eq:H_CM} and \ref{eq:I_CM}

\begin{equation} \label{eq:H_CM}
H_c = \frac{1}{2}\left( p_{xc}^2 + p_{yc}^2 + x_c^2 + y_c^2 \right) - t \mathbb{Re}\left[\frac{z_c}{\sqrt{1-z_c^2}}\textrm{arcsin}(z_c)\right]
\end{equation}

\begin{equation} \label{eq:I_CM}
I_c = (x_c p_{yc} - y_c p_{xc})^2 + p_{xc}^2 + x_c^2 - t\mathbb{Re}\left[\frac{2 x_c}{\sqrt{1-z_c^2}}\textrm{arcsin}(z_c)\right]
\end{equation}

Since the normalization changed these quantities are related to the original Hamiltonian and second invariant by the relations in \ref{eq:DNtoCM}

\begin{equation} \label{eq:DNtoCM}
\begin{split}
	H_{DN} &= c^2 H_c \\
	I_{DN} &= c^4 I_c
\end{split}
\end{equation}

We can plot the potential $U$ with respect to our fully normalized coordinates $x_c,y_x$ in Figure \ref{fig:DNpot}.

\begin{figure} 
	\centering
	\includegraphics[width=0.8\linewidth]{dnPot.pdf}
	\caption{Potential of Danilov-Nagaitsev Integrable System. Yellow arrows indicate direction of potential scaling with $t$-parameter, and the cross indicates the location of the pole where $x=c$}
	\label{fig:DNpot}
\end{figure}

This plot allows us to clearly illustrate the impact of the two arbitrary constants in the DN system. The $c$ parameter describes the locations of singularities in the nonlinear potential, specifically when $x_c = 1$ and $y_c = 0$. The dimensionless $t$ parameter encodes the relative strength of nonlinear insert, and has the strongest impact on the dynamics. Practically, the $t$-parameter is selected to vary in the range $-0.5 \leq t \leq 0$, here a value of $t=0$ recovers our Courant-Snyder case. The impact on the dynamics for further $t$ ranges are considered in \cite{mitchellBifurcationAnalysisNonlinear2020}. 

As the potential satisfies Laplace's equation it can be represented with a string of magnets with properly shaped poles. The details of the implementation of such a magnet are discussed in further chapters.

